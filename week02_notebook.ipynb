{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab6f10c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's load our environment variables and import the required libraries. The `.env` file contains configuration settings that we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9278c5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.13/site-packages (6.0.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (3.10.6)\n",
      "Requirement already satisfied: drawsvg in ./.venv/lib/python3.13/site-packages (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this if packages are not already installed)\n",
    "%pip install python-dotenv requests lxml matplotlib drawsvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d268c921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import dotenv\n",
    "from lxml import html\n",
    "import matplotlib.pyplot as plt\n",
    "import drawsvg as draw\n",
    "import re\n",
    "\n",
    "# Import our custom utilities\n",
    "from scraping_utils import get_url, parse\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f2cf0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables (after mapping):\n",
      "YEAR: 2010\n",
      "URL: https://data.weather.gov.hk/weatherAPI/cis/csvfile/SE/ALL/daily_SE_RF_ALL.csv\n",
      "ROW_XPATH: None\n",
      "COL_XPATH: None\n",
      "MULTICITY_URL: None\n",
      "RAINFALL_URL: https://data.weather.gov.hk/weatherAPI/cis/csvfile/SE/ALL/daily_SE_RF_ALL.csv\n",
      "RAINFALL_STATION_NAME: Kaitak\n",
      "START_YEAR: 2010\n",
      "END_YEAR: 2025\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file (force-reload and map new names to legacy names)\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "# Try to override existing env vars when supported\n",
    "try:\n",
    "    load_dotenv(override=True)\n",
    "except TypeError:\n",
    "    for k, v in dotenv_values('.env').items():\n",
    "        if v is not None:\n",
    "            os.environ[k] = v\n",
    "\n",
    "# Backwards compatibility: if the notebook expects YEAR/URL/etc but we have RAINFALL_URL/START_YEAR, map them\n",
    "# Map only when legacy names are not set to avoid overwriting explicit values\n",
    "if os.getenv('YEAR') is None and os.getenv('START_YEAR') is not None:\n",
    "    os.environ['YEAR'] = os.getenv('START_YEAR')\n",
    "if os.getenv('URL') is None and os.getenv('RAINFALL_URL') is not None:\n",
    "    # Keep legacy URL format using YEAR placeholder if needed\n",
    "    os.environ['URL'] = os.getenv('RAINFALL_URL')\n",
    "# ROW_XPATH/COL_XPATH may not be needed for the CSV flow; leave as-is unless provided\n",
    "# MULTICITY_URL fallback: map from MULTICITY_URL or leave None\n",
    "if os.getenv('MULTICITY_URL') is None and os.getenv('MULTICITY_URL') is None:\n",
    "    pass\n",
    "\n",
    "# Display the loaded and mapped environment variables\n",
    "print(\"Loaded environment variables (after mapping):\")\n",
    "print(f\"YEAR: {os.getenv('YEAR')}\")\n",
    "print(f\"URL: {os.getenv('URL')}\")\n",
    "print(f\"ROW_XPATH: {os.getenv('ROW_XPATH')}\")\n",
    "print(f\"COL_XPATH: {os.getenv('COL_XPATH')}\")\n",
    "print(f\"MULTICITY_URL: {os.getenv('MULTICITY_URL')}\")\n",
    "print(f\"RAINFALL_URL: {os.getenv('RAINFALL_URL')}\")\n",
    "print(f\"RAINFALL_STATION_NAME: {os.getenv('RAINFALL_STATION_NAME')}\")\n",
    "print(f\"START_YEAR: {os.getenv('START_YEAR')}\")\n",
    "print(f\"END_YEAR: {os.getenv('END_YEAR')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89342464",
   "metadata": {},
   "source": [
    "## 2. Web Scraping Utilities\n",
    "\n",
    "Let's examine and demonstrate the utility functions we've created for web scraping. These functions help us fetch web pages and parse different data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ecfa802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_url function:\n",
      "def get_url(url, filename):\n",
      "    if not os.path.exists(filename):\n",
      "        \n",
      "        # set useragent for requests\n",
      "        headers = {\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
      "        \n",
      "        # fetch the page if it doesn't exist\n",
      "        page = requests.get(url, headers=headers)\n",
      "\n",
      "        # save the page to a file\n",
      "        with open(filename, 'w', encoding='UTF8') as f:\n",
      "            f.write(page.text)\n",
      "\n",
      "        page = page.text\n",
      "\n",
      "    else:\n",
      "        # if the page exists, read it from the file\n",
      "        with open(filename, 'r', encoding='UTF8') as f:\n",
      "            page = f.read() \n",
      "            \n",
      "    return page\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "parse function:\n",
      "def parse(page, mode = 'html'):\n",
      "    match mode:\n",
      "        case 'html':\n",
      "            return html.fromstring(page)\n",
      "        case 'json':\n",
      "            return json.loads(page)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our scraping utilities\n",
    "import inspect\n",
    "\n",
    "print(\"get_url function:\")\n",
    "print(inspect.getsource(get_url))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"parse function:\")\n",
    "print(inspect.getsource(parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf29ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching test page...\n",
      "Content length: 429 characters\n",
      "First 200 characters: {\n",
      "  \"slideshow\": {\n",
      "    \"author\": \"Yours Truly\", \n",
      "    \"date\": \"date of publication\", \n",
      "    \"slides\": [\n",
      "      {\n",
      "        \"title\": \"Wake up to WonderWidgets!\", \n",
      "        \"type\": \"all\"\n",
      "      }, \n",
      "      {\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the get_url function with a simple example\n",
    "# This will either fetch from the web or read from cache\n",
    "\n",
    "test_url = \"https://httpbin.org/json\"\n",
    "test_filename = \"test_page.html\"\n",
    "\n",
    "print(\"Fetching test page...\")\n",
    "page_content = get_url(test_url, test_filename)\n",
    "print(f\"Content length: {len(page_content)} characters\")\n",
    "print(f\"First 200 characters: {page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54e0e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON data:\n",
      "{\n",
      "  \"slideshow\": {\n",
      "    \"author\": \"Yours Truly\",\n",
      "    \"date\": \"date of publication\",\n",
      "    \"slides\": [\n",
      "      {\n",
      "        \"title\": \"Wake up to WonderWidgets!\",\n",
      "        \"type\": \"all\"\n",
      "      },\n",
      "      {\n",
      "        \"items\": [\n",
      "          \"Why <em>WonderWidgets</em> are great\",\n",
      "          \"Who <em>buys</em> WonderWidgets\"\n",
      "        ],\n",
      "        \"title\": \"Overview\",\n",
      "        \"type\": \"all\"\n",
      "      }\n",
      "    ],\n",
      "    \"title\": \"Sample Slide Show\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate parsing JSON data\n",
    "json_data = parse(page_content, 'json')\n",
    "print(\"Parsed JSON data:\")\n",
    "print(json.dumps(json_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f03127",
   "metadata": {},
   "source": [
    "## 3. HTML Data Extraction and Processing\n",
    "\n",
    "Now let's work with the actual HTML data that was scraped from the Hong Kong Observatory rainfall information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685de990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HTML file: crawled-page-2010.html\n",
      "HTML file loaded successfully!\n",
      "Content length: 91679 characters\n",
      "First 500 characters:\n",
      "ï»¿ç¸½é¨é (æ¯«ç±³) - åå¾·\n",
      "Total Rainfall (mm) - Kai Tak\n",
      "å¹´/Year,æ/Month,æ¥/Day,æ¸å¼/Value,æ¸æå®æ´æ§/data Completeness\n",
      "2010,1,1,***,\n",
      "2010,1,2,***,\n",
      "2010,1,3,***,\n",
      "2010,1,4,***,\n",
      "2010,1,5,***,\n",
      "2010,1,6,***,\n",
      "2010,1,7,***,\n",
      "2010,1,8,***,\n",
      "2010,1,9,***,\n",
      "2010,1,10,***,\n",
      "2010,1,11,***,\n",
      "2010,1,12,***,\n",
      "2010,1,13,***,\n",
      "2010,1,14,***,\n",
      "2010,1,15,***,\n",
      "2010,1,16,***,\n",
      "2010,1,17,***,\n",
      "2010,1,18,***,\n",
      "2010,1,19,***,\n",
      "2010,1,20,***,\n",
      "2010,1,21,***,\n",
      "2010,1,22,***,\n",
      "2010,1,23,***,\n",
      "2010,1,24,***,\n",
      "2010,1,25,***,...\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the crawled HTML page\n",
    "# Prefer a local processed CSV if available — this skips HTML parsing when you already have data\n",
    "csv_file = 'rainfall_processed.csv'\n",
    "data = []\n",
    "if os.path.exists(csv_file):\n",
    "    print(f'Local CSV found: {csv_file} — loading and skipping HTML parsing')\n",
    "    import csv\n",
    "    with open(csv_file, 'r', encoding='utf8') as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            # Expecting columns: datetime, rainfall_mm, humidity_pct\n",
    "            try:\n",
    "                dt = datetime.datetime.strptime(row.get('datetime','')[:16], '%Y-%m-%d %H:%M')\n",
    "            except Exception:\n",
    "                continue\n",
    "            rainfall = None\n",
    "            humidity = None\n",
    "            if row.get('rainfall_mm') not in (None, ''):\n",
    "                try:\n",
    "                    rainfall = float(row.get('rainfall_mm'))\n",
    "                except Exception:\n",
    "                    rainfall = None\n",
    "            if row.get('humidity_pct') not in (None, ''):\n",
    "                try:\n",
    "                    humidity = float(row.get('humidity_pct'))\n",
    "                except Exception:\n",
    "                    humidity = None\n",
    "            data.append((dt, rainfall, humidity))\n",
    "    print(f'Loaded {len(data)} records from {csv_file}')\n",
    "    # Provide downstream variables expected by later cells\n",
    "    html_content = None\n",
    "    tree = None\n",
    "    rows = []\n",
    "else:\n",
    "    # Load and examine the crawled HTML page\n",
    "    year = int(os.getenv('YEAR', 2024))\n",
    "    filename = os.getenv('FILENAME', \"crawled-page-{year}.html\").format(year=year)\n",
    "\n",
    "    print(f\"Loading HTML file: {filename}\")\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='UTF8') as f:\n",
    "            html_content = f.read()\n",
    "    \n",
    "        print(f\"HTML file loaded successfully!\")\n",
    "        print(f\"Content length: {len(html_content)} characters\")\n",
    "        print(f\"First 500 characters:\\n{html_content[:500]}...\")\n",
    "    else:\n",
    "        print(f\"HTML file {filename} not found. Let's fetch it from the web.\")\n",
    "        \n",
    "        # Use environment variable URL and YEAR for Hong Kong rainfall 2024\n",
    "        url = os.getenv('URL').replace('${YEAR}', str(year))\n",
    "        print(f\"Fetching from: {url}\")\n",
    "        \n",
    "        html_content = get_url(url, filename)\n",
    "        print(f\"Content fetched and saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85fd4b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 rows in the HTML table\n",
      "\n",
      "First 3 rows structure:\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "tree = parse(html_content, 'html')\n",
    "\n",
    "# Extract table structure using XPath\n",
    "# Use fallback XPath if environment variables are not set\n",
    "row_xpath = os.getenv('ROW_XPATH')\n",
    "col_xpath = os.getenv('COL_XPATH')\n",
    "if row_xpath is None:\n",
    "    # Fallback: select all <tr> elements\n",
    "    row_xpath = '//tr'\n",
    "if col_xpath is None:\n",
    "    # Fallback: select all <td> and <th> elements within a row\n",
    "    col_xpath = './td | ./th'\n",
    "\n",
    "rows = tree.xpath(row_xpath)\n",
    "print(f\"Found {len(rows)} rows in the HTML table\")\n",
    "\n",
    "# Examine the first few rows\n",
    "print(\"\\nFirst 3 rows structure:\")\n",
    "for i, row in enumerate(rows[:3]):\n",
    "    columns = row.xpath(os.getenv('COL_XPATH'))\n",
    "    column_texts = [col.text_content().strip() for col in columns]\n",
    "    print(f\"Row {i+1}: {column_texts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f93cb",
   "metadata": {},
   "source": [
    "## 4. CSV Data Handling for Rainfall\n",
    "\n",
    "Let's process the rainfall data and create a CSV file, then load and analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9fab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rainfall and humidity data...\n",
      "\n",
      "Processed 0 rainfall/humidity data points\n"
     ]
    }
   ],
   "source": [
    "# Process rainfall and humidity data\n",
    "# We'll attempt to extract both rainfall (mm) and humidity (%) when available\n",
    "data = []  # list of tuples: (datetime, rainfall_mm_or_None, humidity_pct_or_None)\n",
    "row_num = 0\n",
    "\n",
    "print(\"Processing rainfall and humidity data...\")\n",
    "\n",
    "for row in tree.xpath(row_xpath):\n",
    "    columns = row.xpath(col_xpath)\n",
    "    columns = [column.text_content().strip() if column.text_content() is not None else '' for column in columns]\n",
    "    row_string = \" \\\n",
    ",\n",
    ",\n",
    ",\n",
    "\":\n",
    "        continue\n",
    "    \n",
    "    row_num += 1\n",
    "    \n",
    "    # Only log first few rows for demonstration\n",
    "    if row_num <= 5:\n",
    "        print(f'Row {row_num}: {row_string}')\n",
    "    \n",
    "    # Skip header or invalid rows\n",
    "    if len(columns) < 3 or not columns[0].lstrip('0').isdigit():\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        month = int(columns[0])\n",
    "        day = int(columns[1])\n",
    "        \n",
    "        # Iterate through the remaining columns trying to pair time, rainfall and possibly humidity\n",
    "        # Common layouts: [MM, DD, HHMM, mm, HHMM, mm, ...] or sometimes humidity appears nearby like '45%']\n",
    "        for i in range(2, len(columns)):\n",
    "            col = columns[i]\n",
    "            # Identify time-like entries (HHMM or HMM)\n",
    "            time_digits = re.sub(r'[^0-9]', '', col)\n",
    "            if len(time_digits) >= 3 and len(time_digits) <= 4:\n",
    "                # Normalize to HHMM if needed\n",
    "                if len(time_digits) == 3:\n",
    "                    time_digits = '0' + time_digits\n",
    "                if not time_digits.isdigit():\n",
    "                    continue\n",
    "                hour = int(time_digits[:2])\n",
    "                minute = int(time_digits[2:])\n",
    "                # Build datetime, guard against invalid hours/minutes\n",
    "                try:\n",
    "                    dt = datetime.datetime(year, month, day, hour, minute)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                # Attempt to read next columns for rainfall and humidity\n",
    "                rainfall = None\n",
    "                humidity = None\n",
    "                # Next column often contains rainfall amount\n",
    "                if i+1 < len(columns):\n",
    "                    next_col = columns[i+1]\n",
    "                    # Extract numeric rainfall value (allow decimal)\n",
    "                    val = re.sub(r'[^0-9.]', '', next_col)\n",
    "                    if val != '':\n",
    "                        try:\n",
    "                            rainfall = float(val)\n",
    "                        except ValueError:\n",
    "                            rainfall = None\n",
    "                # Look ahead one more column for humidity if present (e.g., '45%')\n",
    "                if i+2 < len(columns):\n",
    "                    maybe_hum = columns[i+2]\n",
    "                    hum_digits = re.sub(r'[^0-9.]', '', maybe_hum)\n",
    "                    if hum_digits != '':\n",
    "                        try:\n",
    "                            humidity = float(hum_digits)\n",
    "                        except ValueError:\n",
    "                            humidity = None\n",
    "                # As a fallback, search the same row for any value containing '%'\n",
    "                if humidity is None:\n",
    "                    for c in columns:\n",
    "                        if '%' in c:\n",
    "                            h = re.sub(r'[^0-9.]', '', c)\n",
    "                            if h != '':\n",
    "                                try:\n",
    "                                    humidity = float(h)\n",
    "                                    break\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "                # Append record only if we have at least rainfall or humidity\n",
    "                if rainfall is not None or humidity is not None:\n",
    "                    data.append((dt, rainfall, humidity))\n",
    "                    if len(data) <= 10:\n",
    "                        print(f'{dt} - rainfall={rainfall} mm, humidity={humidity} %')\n",
    "    except (ValueError, IndexError) as e:\n",
    "        # Skip problematic rows\n",
    "        continue\n",
    "\n",
    "print(f\"\\nProcessed {len(data)} rainfall/humidity data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8344a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CSV file: rainfall_processed.csv\n",
      "CSV file created with 0 records\n",
      "\n",
      "First 10 lines of CSV:\n",
      "datetime,rainfall_mm,humidity_pct\n"
     ]
    }
   ],
   "source": [
    "# Create CSV file for rainfall and humidity\n",
    "csv_filename = 'rainfall_processed.csv'\n",
    "\n",
    "print(f\"Creating CSV file: {csv_filename}\")\n",
    "\n",
    "with open(csv_filename, 'w') as f:\n",
    "    f.write('datetime,rainfall_mm,humidity_pct\\n')  # Header\n",
    "    for record in data:\n",
    "        dt_str = record[0].strftime(\"%Y-%m-%d %H:%M\")\n",
    "        rainfall_val = '' if record[1] is None else f'{record[1]:.2f}'\n",
    "        humidity_val = '' if record[2] is None else f'{record[2]:.1f}'\n",
    "        f.write(f'{dt_str},{rainfall_val},{humidity_val}\\n')\n",
    "\n",
    "print(f\"CSV file created with {len(data)} records\")\n",
    "\n",
    "# Read and display first few lines of the CSV\n",
    "with open(csv_filename, 'r') as f:\n",
    "    lines = f.readlines()[:10]\n",
    "    print(\"\\nFirst 10 lines of CSV:\")\n",
    "    for line in lines:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500653b",
   "metadata": {},
   "source": [
    "## 5. Daily Mean Wind Speed All Year - Kai Tak\n",
    "\n",
    "We'll fetch the Hong Kong Observatory daily wind CSV and extract the daily mean wind speed for the Kai Tak (KaiTak) station.\n",
    "\n",
    "The notebook will look for environment variables `WIND_URL` and `WIND_STATION_NAME`. If they are not present, it will fall back to sensible defaults and the previously used `RAINFALL_` variables where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "faea1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching wind CSV from: https://data.weather.gov.hk/weatherAPI/cis/csvfile/SE/ALL/daily_SE_RF_ALL.csv\n",
      "Filtering station: Kaitak\n",
      "CSV header: ï»¿å¹³åé¢¨é (å\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     24\u001b[39m kai_tak_data = []\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# HKO CSV typically contains a 'station' or 'Station' field and date fields like 'yyyy-mm-dd' or 'date'\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# We'll perform a case-insensitive check for the station name in any of the station-like fields\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     station_fields = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m row.keys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mstation\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mstn\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k.lower()]\n\u001b[32m     30\u001b[39m     station_val = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sf \u001b[38;5;129;01min\u001b[39;00m station_fields:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Daily Mean Wind Speed for Kai Tak (from HKO CSV)\n",
    "# Environment variables: WIND_URL, WIND_STATION_NAME, START_YEAR, END_YEAR\n",
    "wind_url = os.getenv('WIND_URL') or os.getenv('RAINFALL_URL') or 'https://data.weather.gov.hk/cis/csvfile/SE/ALL/daily_SE_WSPD_ALL.csv'\n",
    "wind_station = os.getenv('WIND_STATION_NAME') or os.getenv('RAINFALL_STATION_NAME') or 'KaiTak'\n",
    "start_year = int(os.getenv('START_YEAR', 2010))\n",
    "end_year = int(os.getenv('END_YEAR', 2025))\n",
    "\n",
    "print(f\"Fetching wind CSV from: {wind_url}\")\n",
    "print(f\"Filtering station: {wind_station}\")\n",
    "\n",
    "# Download the CSV (uses caching behavior of get_url if implemented)\n",
    "csv_filename = 'daily_SE_WSPD_ALL.csv'\n",
    "csv_text = get_url(wind_url, csv_filename)\n",
    "\n",
    "# Parse CSV lines\n",
    "lines = csv_text.splitlines()\n",
    "header = lines[0] if lines else ''\n",
    "print(f\"CSV header: {header[:200]}\")\n",
    "\n",
    "import csv\n",
    "reader = csv.DictReader(lines)\n",
    "\n",
    "# kai_tak_data will be list of dicts: {'date': date_obj, 'mean_wspd': float_or_None, 'station': station_name}\n",
    "kai_tak_data = []\n",
    "\n",
    "for row in reader:\n",
    "    # HKO CSV typically contains a 'station' or 'Station' field and date fields like 'yyyy-mm-dd' or 'date'\n",
    "    # We'll perform a case-insensitive check for the station name in any of the station-like fields\n",
    "    station_fields = [k for k in row.keys() if 'station' in k.lower() or 'stn' in k.lower()]\n",
    "    station_val = None\n",
    "    for sf in station_fields:\n",
    "        if row.get(sf):\n",
    "            station_val = row.get(sf).strip()\n",
    "            break\n",
    "    # Some CSVs include station as a separate column, others include station in a combined column; fallback to searching all values\n",
    "    if station_val is None:\n",
    "        # look for exact station name anywhere in the row values\n",
    "        for v in row.values():\n",
    "            if v and wind_station.lower() in v.lower():\n",
    "                station_val = v.strip()\n",
    "                break\n",
    "    if station_val is None:\n",
    "        continue\n",
    "    # Parse date: try common column names\n",
    "    date_str = None\n",
    "    for date_key in ['date', 'Date', 'obs_time', 'yyyy-mm-dd', 'yyyymmdd']:\n",
    "        if date_key in row and row[date_key]:\n",
    "            date_str = row[date_key].strip()\n",
    "            break\n",
    "    if date_str is None:\n",
    "        # attempt to find a field that looks like a date\n",
    "        for k, v in row.items():\n",
    "            if v and re.match(r'\\d{4}-\\d{2}-\\d{2}', v):\n",
    "                date_str = v.strip()\n",
    "                break\n",
    "    if date_str is None:\n",
    "        continue\n",
    "    # Parse mean wind speed value: look for 'mean', 'avg', 'wspd' in column names\n",
    "    wspd_fields = [k for k in row.keys() if any(x in k.lower() for x in ['mean', 'avg', 'wspd', 'w_speed', 'wind'])]\n",
    "    wspd_val = None\n",
    "    for wf in wspd_fields:\n",
    "        v = row.get(wf)\n",
    "        if v and v.strip() != '':\n",
    "            # strip non-numeric characters\n",
    "            num = re.sub(r'[^0-9\\.]', '', v)\n",
    "            if num != '':\n",
    "                try:\n",
    "                    wspd_val = float(num)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    wspd_val = None\n",
    "    # fallback: check common column names explicitly\n",
    "    if wspd_val is None:\n",
    "        for explicit in ['MeanWindSpeed', 'MEAN_WSPD', 'Mean_Wspd', 'WSPD']:\n",
    "            if explicit in row and row[explicit].strip() != '':\n",
    "                num = re.sub(r'[^0-9\\.]', '', row[explicit])\n",
    "                if num != '':\n",
    "                    try:\n",
    "                        wspd_val = float(num)\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        wspd_val = None\n",
    "    # Parse the date into a datetime.date where possible\n",
    "    try:\n",
    "        date_obj = datetime.datetime.strptime(date_str[:10], '%Y-%m-%d').date()\n",
    "    except Exception:\n",
    "        # skip unparsable dates\n",
    "        continue\n",
    "    # Filter by year range\n",
    "    if date_obj.year < start_year or date_obj.year > end_year:\n",
    "        continue\n",
    "    # Match station name loosely (case-insensitive substring)\n",
    "    if station_val is not None and wind_station.lower() in station_val.lower():\n",
    "        kai_tak_data.append({'date': date_obj, 'mean_wspd': wspd_val, 'station': station_val})\n",
    "\n",
    "print(f\"Found {len(kai_tak_data)} records for station '{wind_station}' between {start_year} and {end_year}\")\n",
    "if kai_tak_data:\n",
    "    print('Sample records (first 10):')\n",
    "    for r in kai_tak_data[:10]:\n",
    "        print(r)\n",
    "else:\n",
    "    print('No matching records found. Check WIND_URL and WIND_STATION_NAME environment variables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12927dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer the locally generated CSV if it exists to avoid re-downloading the whole HKO file\n",
    "local_csv = f'kaitak_wind_{start_year}_{end_year}.csv'\n",
    "kai_tak_data = []\n",
    "if os.path.exists(local_csv):\n",
    "    print(f'Loading local processed CSV: {local_csv}')\n",
    "    import csv\n",
    "    with open(local_csv, 'r') as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for row in rdr:\n",
    "            try:\n",
    "                d = datetime.datetime.strptime(row['date'], '%Y-%m-%d').date()\n",
    "            except Exception:\n",
    "                continue\n",
    "            val = row.get('mean_wspd')\n",
    "            wspd = float(val) if val not in (None, '') else None\n",
    "            kai_tak_data.append({'date': d, 'mean_wspd': wspd, 'station': row.get('station')})\n",
    "else:\n",
    "    print('Local processed CSV not found; falling back to parsing the fetched HKO CSV')\n",
    "    # At this point `reader` and `lines` may be available from the previous cell; if not, re-fetch\n",
    "    try:\n",
    "        reader\n",
    "    except NameError:\n",
    "        csv_text = get_url(wind_url, 'daily_SE_WSPD_ALL.csv')\n",
    "        lines = csv_text.splitlines()\n",
    "        header = lines[0] if lines else ''\n",
    "        import csv\n",
    "        reader = csv.DictReader(lines)\n",
    "    # Fallback parsing: scan DictReader rows for station and wind speed data\n",
    "    kai_tak_data = []\n",
    "    for row in reader:\n",
    "        # attempt to build date and value from common dict fields\n",
    "        date_str = row.get('date') or row.get('Date') or row.get('yyyy-mm-dd') or ''\n",
    "        if not date_str:\n",
    "            # try Year,Month,Day fields\n",
    "            try:\n",
    "                y = int(row.get('Year', '0'))\n",
    "                m = int(row.get('Month', '0'))\n",
    "                d = int(row.get('Day', '0'))\n",
    "                date_obj = datetime.date(y, m, d)\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date_str[:10], '%Y-%m-%d').date()\n",
    "            except Exception:\n",
    "                continue\n",
    "        v = row.get('Value') or row.get('value') or row.get('mean_wspd') or ''\n",
    "        wspd = None\n",
    "        if v not in (None, ''):\n",
    "            try:\n",
    "                wspd = float(re.sub(r'[^0-9\\.-]', '', v))\n",
    "            except Exception:\n",
    "                wspd = None\n",
    "        # check station cell\n",
    "        station_val = row.get('station') or row.get('Station') or ''\n",
    "        if station_val and wind_station.lower() in station_val.lower():\n",
    "            kai_tak_data.append({'date': date_obj, 'mean_wspd': wspd, 'station': station_val})\n",
    "\n",
    "print(f'Loaded kai_tak_data records: {len(kai_tak_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5043a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of fetched city data:\n",
      "\n",
      "City 1: Hong Kong\n",
      "  Max Temperature: 18.7\n",
      "  Min Temperature: 14.6\n",
      "  Rainfall: 33.2\n",
      "\n",
      "City 2: Lisboa\n",
      "  Max Temperature: 14.5\n",
      "  Min Temperature: 8.2\n",
      "  Rainfall: 109.6\n",
      "\n",
      "City 3: Porto\n",
      "  Max Temperature: 13.5\n",
      "  Min Temperature: 5.1\n",
      "  Rainfall: 170.8\n"
     ]
    }
   ],
   "source": [
    "# Display summary of Kai Tak daily mean wind speed data\n",
    "if kai_tak_data:\n",
    "    print(\"Summary of Kai Tak wind data:\")\n",
    "    # Show basic stats\n",
    "    speeds = [r['mean_wspd'] for r in kai_tak_data if r['mean_wspd'] is not None]\n",
    "    dates = [r['date'] for r in kai_tak_data]\n",
    "    print(f\"Records: {len(kai_tak_data)} (with numeric wind speeds: {len(speeds)})\")\n",
    "    if speeds:\n",
    "        print(f\"Mean wind speed: {sum(speeds)/len(speeds):.2f} (units as in source)\")\n",
    "        print(f\"Min: {min(speeds):.2f}, Max: {max(speeds):.2f}\")\n",
    "    print('Sample (first 10)')\n",
    "    for r in kai_tak_data[:10]:\n",
    "        print(f\"  {r['date']} - {r['station']} - mean_wspd={r['mean_wspd']}\")\n",
    "else:\n",
    "    print(\"No Kai Tak wind data was successfully fetched.\")\n",
    "    print(\"Check WIND_URL, WIND_STATION_NAME, and network connectivity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f51137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kai_tak_data not found or empty. Run the previous cell to fetch data first.\n"
     ]
    }
   ],
   "source": [
    "# Plot Kai Tak daily mean wind speed (if available)\n",
    "if 'kai_tak_data' in globals() and kai_tak_data:\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Time series (only numeric values)\n",
    "    numeric = [r for r in kai_tak_data if r['mean_wspd'] is not None]\n",
    "    if numeric:\n",
    "        dates = [r['date'] for r in numeric]\n",
    "        speeds = [r['mean_wspd'] for r in numeric]\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(dates, speeds, '-o', markersize=3)\n",
    "        plt.title('Kai Tak - Daily Mean Wind Speed')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Mean Wind Speed')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # Histogram\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.hist(speeds, bins=30, edgecolor='black', alpha=0.7)\n",
    "        plt.title('Distribution of Daily Mean Wind Speed - Kai Tak')\n",
    "        plt.xlabel('Mean Wind Speed')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No numeric mean wind speed values to plot')\n",
    "else:\n",
    "    print('kai_tak_data not found or empty. Run the previous cell to fetch data first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f130235",
   "metadata": {},
   "source": [
    "## 6. Rainfall Data Visualization\n",
    "\n",
    "Now let's create visualizations of our rainfall data using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a614ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rainfall/humidity data available for plotting\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for plotting (rainfall and humidity)\n",
    "if data:\n",
    "    plot_data = []  # list of (dt, rainfall_float_or_None, humidity_float_or_None)\n",
    "    for dt, rainfall, humidity in data:\n",
    "        # rainfall and humidity are already numeric or None\n",
    "        plot_data.append((dt, rainfall, humidity))\n",
    "    \n",
    "    print(f\"Prepared {len(plot_data)} data points for plotting\")\n",
    "    \n",
    "    # Separate series\n",
    "    dates = [p[0] for p in plot_data]\n",
    "    rainfall_values = [p[1] for p in plot_data]\n",
    "    humidity_values = [p[2] for p in plot_data]\n",
    "    \n",
    "    # Compute ranges where data exists\n",
    "    valid_rain = [v for v in rainfall_values if v is not None]\n",
    "    valid_hum = [h for h in humidity_values if h is not None]\n",
    "    if valid_rain:\n",
    "        print(f\"Rainfall range: {min(valid_rain):.2f} mm to {max(valid_rain):.2f} mm\")\n",
    "    if valid_hum:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
